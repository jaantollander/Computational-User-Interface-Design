---
title: A11.1 - Modifying the Bandits Problem and the \(Îµ\)-Greedy Solver
author: Jaan Tollander de Balsch - 452056
date: \today
---
## Multi-Armed Bandit
A tuple of \(âŸ¨A,RâŸ©\), where:

* \(A\) is a set of **actions**.
* \(R\) is a **reward function**.

At each time step \(t\), we take an action \(aâˆˆA\) on one slot machine and receive a rewards \(râˆˆR\).


## Reward and Regret
**Action-Value** is the mean reward for action \(a\)
\[
Q(a) = ğ”¼[râˆ£a]
\]

**Optimal reward** of the optimal action \(a^*\)
\[
Q(a^*) = \max_{aâˆˆA} Q(a)
\]

The **regret** is the opportunity loss for one step
\[
l_t = ğ”¼[Q(a^*)-Q(a_t)]
\]

## Epsilon-Greedy
* Eplore with probability \(Îµ\)
* Eploit with probability \(1-Îµ\)
* Estimated action-value for arm \(a\) is \[\hat{Q}_t(a)=\frac{1}{N_t(a)}\sum_{t=1}^T r_t ğŸ™[a_t=a]\]
* Arm to exploit: \[a_t^*=\operatorname{argmax}_{aâˆˆA}\hat{Q}_t(a)\]


## Bernoulli-Uniform Bandit
The **reward** \(r\) is a random number sampled from a probability distribution. The distributions are assumed to be known and independent, but the parameters unknown. For example, the reward for *Bernoulli-Uniform* bandit is
\[
râˆ¼\operatorname{Bernoulli}(Î¸)â‹…\operatorname{Uniform}(l,h)
\]
where \(Î¸âˆˆ[0,1]\) and \(l,hâˆˆâ„, lâ‰¤h\). The expected reward for action \(a\) is
\[
\begin{aligned}
ğ”¼[râˆ£a] & = ğ”¼[\operatorname{Bernoulli}(Î¸)]â‹…ğ”¼[\operatorname{Uniform}(l,h)] \\ & =
Î¸ â‹… (l+h)/2.
\end{aligned}
\]

Other distributions could be used in similar way.


## Epsilon-Greedy with Decreasing Epsilon
The Epsilon-Greedy should be modified such that initially it explores more in order to estimate the action values and as time increases and the evidence for action value estimates increase, the epsilon should decrease such that the best arm is exploited more often.

A naive formulation of epsilon \(Îµ_t\) is to formulate it as a decreasing function \(Îµ_{t+1}<Îµ_t\) of time \(t\) starting from initial epsilon \(Îµ_0\) decreasing towards zero as time tends towards infinity \(tâ†’âˆ\). For example,
\[
Îµ_t=Îµ_0/t^{c}
\]
where \(câˆˆâ„\) is a parameter that controls the rate of the decrease. For larger number of bandits the rate of decrease should be set lower. If \(c=0\) the approach reduces to normal Epsilon-Greedy.

<!-- **NOTE**: The confidence interval approach is the non-naive -- but not epsilon-greedy -- approach. It takes into account the amount of times each action has been taken \(N_t(a)\). -->
